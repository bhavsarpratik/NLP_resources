{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA in Gensim\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# want to make clean words and return a list of tokens\n",
    "\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCREEN_NAME', 'said', 'the', '#', 'chicken', 'was', 'at', 'the', '#', 'junkyard', '.', 'see', 'URL', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = '@bob said the #chicken was at the #junkyard. See http://www.jonathanmugan.com.'\n",
    "out_tokens = tokenize(sent)\n",
    "print(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we want to lemmatize so dogs goes to dog and ran goes to run\n",
    "# Lemmatization means to get the \"dictionary entry\" for a word\n",
    "\n",
    "# Some documentation here http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else: \n",
    "        return lemma\n",
    "    \n",
    "# or can use this\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs dog dog\n",
      "ran run ran\n",
      "discouraged discourage discouraged\n"
     ]
    }
   ],
   "source": [
    "for w in ['dogs','ran','discouraged']:\n",
    "    print(w,get_lemma(w),get_lemma2(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'going', 'restaurant', 'hamburger']\n"
     ]
    }
   ],
   "source": [
    "sent = 'I enjoy going to restaurants to eat hamburgers.'\n",
    "print(prepare_text_for_lda(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['watch', 'great', 'beauty', 'still', 'morning', 'world', 'begin']\n",
      "['recently', 'attend', 'wedding', 'priest', 'sound', 'exactly', 'christopher', 'walken']\n",
      "['recently', 'watch', 'transcendence', 'appear', 'johnny', 'assume', 'scientist', 'soulless', 'automaton', 'play']\n",
      "['could', 'fully', 'trust', 'documentary', 'filmmaker', 'canâ€™t', 'expect', 'people', 'spend', 'something', 'report']\n",
      "['defensive', 'writing', 'write', 'point', 'across', 'avoid', 'people', 'dismiss', 'mention']\n",
      "['twitter', 'whale', 'funny', 'appreciate', 'annoyance']\n",
      "['interest', 'impression', 'world', 'movie', 'still', 'picture']\n",
      "['everything', 'design', 'human', 'improvable', 'SCREEN_NAME']\n",
      "['dream', 'feel', 'people', 'experience']\n",
      "['learning', 'take', 'bravery', 'willing', 'stupid', 'question']\n",
      "['install', 'software', 'spit', 'page', 'error', 'everything', 'sucessfully', 'instal']\n",
      "['front', 'midnight', 'stroll', 'snake', 'sitting', 'looking', 'guess', 'watch', 'movie', 'instead']\n",
      "['computer', 'enjoy', 'learning', 'almost', 'suffering', 'madly', 'spin', 'quixotic', 'effort']\n",
      "['determinist', 'insurance', 'company', 'insist', 'conditions', 'preexist', 'headline', 'onion']\n",
      "['white', 'collar', 'people', 'understand', 'degree', 'manual', 'labor', 'chew', 'worker', 'body']\n",
      "['learning', 'machine', 'learning', 'always', 'thought', 'kernel', 'people', 'little']\n",
      "['delta', 'prefer', 'memphis', 'atlanta', 'memphis', 'airport', 'smell', 'bacon']\n",
      "['leave', 'country', 'first', 'learn', 'country', 'things', 'could']\n",
      "['evolutionary', 'perspective', 'surprise', 'sleep', 'piece', 'whatever', 'happen', 'along']\n",
      "['remember', 'kaepas', 'sol', 'show', 'grandpa', 'proud', 'thrift', 'need', 'cardboard']\n",
      "['colombia', 'seize', 'cocaine', 'disguise', 'printer', 'probably', 'higher', 'street', 'value', 'printer']\n",
      "['binding', 'drive', 'would', 'windows', 'linux', 'sometimes', 'within']\n",
      "['school', 'wonder', 'life']\n",
      "['look', 'computer', 'remind']\n",
      "['found', 'textbook']\n",
      "['night', 'finally', 'finish', 'watching', 'america', 'encompass', 'event', 'lifetime']\n",
      "['move', 'texas', 'breakfast', 'ask', 'want', 'gravy', 'look', 'stupefy']\n",
      "['people', 'cart', 'allow', 'sidewalk', 'school', 'walking', 'propel', 'motorcycle']\n",
      "['fix', 'unicode', 'probably', 'common', 'commit', 'message']\n",
      "['thing', 'move', 'move', 'office', 'seeing', 'knock', 'brain', 'autopilot', 'alive']\n",
      "['twitter', 'transform', 'years', 'stream', 'idea', 'stream', 'pointer', 'idea']\n",
      "['machine', 'intelligent', 'definition', 'intelligence', 'thermostat', 'reasoning', 'first', 'principle']\n",
      "['forget', 'return', 'redbox', 'movie', 'beginning', 'think', 'absent', 'mindedness', 'entire', 'business', 'model']\n",
      "['years', 'worry', 'getting', 'eat', 'preditors', 'concern', 'getting']\n",
      "['finally', 'quest', 'thousand', 'years', 'years', 'years']\n",
      "['finish', 'laugh', 'monster', 'denis', 'johnson', 'love', 'star', 'amazon', 'wrong', 'people']\n",
      "['today', 'howmydaughterplaysstarwars']\n",
      "['kinda', 'weird', 'church', 'using', 'facial', 'recognition', 'track', 'member']\n",
      "['fiction', 'author', 'scene', 'quickly', 'picture', 'paint', 'experience']\n",
      "['world', 'fraudsters', 'swindler', 'nefarious', 'cranberry', 'pepita', 'salad', 'calorie']\n",
      "['every', 'thing', 'today', 'problem', 'operand', 'broadcast', 'header', 'file', 'found', 'deploy', 'refuse', 'dance']\n",
      "['provide', 'comprehensible', 'introduction', 'confuse', 'comprehensive', 'opposite']\n",
      "['looking', 'around', 'bedroom', 'caveman', 'seeing', 'first', 'picture', 'would', 'little', 'people', 'trap', 'box']\n",
      "['message', 'start', 'freak']\n",
      "['people', 'throw', 'everywhere', 'paris', 'battle', 'cigarette', 'butt']\n",
      "['finally', 'watch', 'latest', 'realize', 'movie']\n",
      "['beautiful', 'picture']\n",
      "['every', 'computer', 'interface', 'machine', 'learning', 'embed', 'learn', 'preference', 'stretching']\n",
      "['funny', 'crying', 'spill', 'refer', 'lose', 'precious', 'resource', 'paper', 'towel']\n",
      "['grocery', 'store', 'suddenly', 'looking', 'weird', 'turn', 'putting', 'stuff', 'basket']\n",
      "['baking', 'seem', 'reasonable', 'thing', 'ingredient', 'whole', 'food', 'treat', 'south']\n",
      "['versus', 'pretentious', 'correct']\n",
      "['abbreviation', 'devil']\n",
      "['pandora', 'finally', 'lyric', 'song', 'hear', 'nauseam', '1980s', 'surprisingly']\n",
      "['daughter', 'angry', 'going', 'shoes']\n",
      "['latest', 'quick', 'scheme', 'every', 'want', 'quick', 'money']\n",
      "['song', 'proclaim', 'better', 'nothing', 'comfortably', 'pretty']\n",
      "['stays', 'forever', 'leverage', 'property', 'broth', 'base', 'energy', 'infrastructure']\n",
      "['italian', 'call', 'original', 'custom', 'anything', 'besides', 'italian', 'sound', 'pretentious', 'false']\n",
      "['bucket', 'popcorn', 'matter', 'movie', 'happy']\n",
      "['people', 'something', 'special', 'talk', 'girl', 'richards', 'school---they', 'memory', 'internet']\n",
      "['actually', 'scratch', 'nothing', 'truly', 'happen', 'great', 'allow', 'clean', 'reset']\n",
      "['anyone', 'amount', 'complexity', 'computer', 'operate', 'system', 'compare', 'biological']\n",
      "['people', 'enjoy', 'parenthetical', 'thinking', 'scatter', 'novel', 'association', 'others', 'linear', 'story']\n",
      "['photo', 'surprisingly', 'conveying', 'person', 'essence', 'realize', 'photo', 'point', 'person', 'exist']\n",
      "['could', 'social', 'medium', 'client', 'platitude', 'filter', 'looking', 'never', 'dream']\n",
      "['madness', 'come', 'tomorrow']\n",
      "['remember', 'windows', 'always', 'worry', 'unused', 'desktop', 'icon']\n",
      "['airline', 'reward', 'grocery', 'store', 'reward', 'opposite', 'airline', 'reward', 'reward', 'flight', 'grocery', 'store', 'reward', 'punishment']\n",
      "['funny', 'negotiation', 'first', 'person', 'specify', 'number', 'lose', 'first', 'person', 'establish', 'number', 'anchor']\n",
      "['understand', 'nightmare', 'teach', 'brain', 'simulate', 'danger', 'annoying', 'dream', 'running', 'barefoot', 'pebble']\n",
      "['successfully', 'resurrect', 'laptop', 'installing', 'ubuntu']\n",
      "['future', 'waste', 'fixing', 'toilet']\n",
      "['kitchen', 'currently', 'close', 'luckily', 'strategic', 'coffee', 'reserve', 'office']\n",
      "['access', 'virtual', 'machine', 'remote', 'desktop', 'indeed', 'turtle']\n",
      "['commercial', 'mcdonald', 'sugary', 'coffee', 'drink', 'distinctly', 'remind', 'heroin', 'injection', 'scene', 'trainspotting']\n",
      "['warning', 'danger', 'heart', 'disease', 'accident', 'cancer', 'nobody', 'talks', 'stairs']\n",
      "['require', 'inspirational', 'quote', 'consider', 'opportunity', 'costs', 'shot', 'could']\n",
      "['think', 'people', 'would', 'surprise', 'learn', 'bigfoot', 'indiana']\n",
      "['break', 'dream', 'would', 'great', 'breakfast', 'cereal', 'hipsters', 'would']\n",
      "['watch', 'wormhole', 'instead', 'monster', 'closet', 'afraid', 'fabric', 'space']\n",
      "['people', 'regrets', 'always', 'wonder', 'delude', 'pay', 'attention']\n",
      "['surprisingly', 'search', 'feature', 'google', 'access', 'search', 'expert']\n",
      "['stick', 'video', 'google', 'solution', 'welcome', 'adult', 'world']\n",
      "['coding', 'elegance', 'beauty', 'beauty', 'sometimes', 'elegant', 'opaque', 'opaque']\n",
      "['believe', 'found', 'guardian', 'angel']\n",
      "['google', 'internet', 'internet']\n",
      "['playing', 'mario', 'brain', 'limited', 'wonder', 'siphon', 'store', 'dynamics', 'flying', 'mushroom']\n",
      "['really', 'believe', 'people', 'second', 'commercial', 'watch', 'second', 'video']\n",
      "['employee', 'tempt', 'string', 'toomuchcomputers']\n",
      "['austin', 'black', 'vortex', 'mopac', 'lamar', 'together', 'every']\n",
      "['chatbot', 'boring', 'things', 'stupid', 'fascinate', 'handsome']\n",
      "['seem', 'lately', 'situation', 'hammer', 'problem', 'problem']\n",
      "['bummer', 'getting', 'older', 'already', 'every', 'movie', 'come']\n",
      "['tire', 'bedtime', 'middle', 'afternoon']\n",
      "['funny', 'quickly', 'place', 'probably', 'come', 'hunter', 'gatherer']\n",
      "['notice', 'never', 'staple']\n",
      "['feeling', 'positive', 'start', 'reading', 'things', 'really', 'going', 'sell', 'better']\n",
      "['itunes', 'music', 'place', 'brain', 'first', 'itunes', 'music', 'belong']\n",
      "['normally', 'drink', 'going', 'regret', 'reason', 'really', 'really', 'want', 'mountain']\n",
      "['anyone', 'tell', 'conditioning', 'pittsburgh', 'likely', 'pants']\n",
      "['proofread', 'writing', 'unsatisfying', 'longer', 'seem', 'novel', 'memory']\n",
      "['amaze', 'practice', 'action', 'would', 'repeat', 'love', 'drove']\n",
      "['comment', 'bottom', 'article', 'recaptchas', 'code', 'prove', 'human']\n",
      "['SCREEN_NAME', '5-year', 'critic', 'agree', 'movie', 'better', 'viewing']\n",
      "['forget', 'bring', 'lunch', 'solution', 'lunch', 'reader', 'alarm', 'lunch']\n",
      "['dream', 'learning', 'simulate', 'experience', 'dream', 'place', 'dream', 'alter', 'familiar', 'place']\n",
      "['arrive', 'place', 'memory', 'first']\n",
      "['nobody', 'thought', 'gsexylove', 'would']\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "import random\n",
    "text_data = []\n",
    "with open('jonathan_mugan_tweets.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .95:\n",
    "            print(tokens)\n",
    "        text_data.append(tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary from the data\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "\n",
    "# Warning message shows that you can also do lemmatization through Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the corpus and dictionary, we will use these in another video to visualize\n",
    "import pickle\n",
    "pickle.dump( corpus, open( \"corpus.pkl\", \"wb\" ) )\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.014*\"remember\" + 0.013*\"could\" + 0.012*\"amaze\" + 0.008*\"funny\"')\n",
      "(1, '0.011*\"people\" + 0.009*\"seem\" + 0.009*\"think\" + 0.008*\"funny\"')\n",
      "(2, '0.009*\"people\" + 0.007*\"funny\" + 0.007*\"really\" + 0.007*\"never\"')\n",
      "(3, '0.013*\"dream\" + 0.012*\"would\" + 0.010*\"want\" + 0.010*\"watch\"')\n",
      "(4, '0.008*\"child\" + 0.008*\"coffee\" + 0.007*\"people\" + 0.007*\"going\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1), (191, 1)]\n",
      "[(0, 0.068171450237185072), (1, 0.068203317548867867), (2, 0.066931796547873221), (3, 0.72843889002983986), (4, 0.068254545636233885)]\n"
     ]
    }
   ],
   "source": [
    "# try a new document\n",
    "# we see it is mostly topic 3\n",
    "new_doc = 'I watch movies.'\n",
    "new_doc = prepare_text_for_lda(new_doc)\n",
    "new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "print(new_doc_bow)\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.008*\"dream\" + 0.007*\"never\" + 0.007*\"could\" + 0.006*\"things\"')\n",
      "(1, '0.010*\"always\" + 0.007*\"going\" + 0.007*\"coffee\" + 0.005*\"could\"')\n",
      "(2, '0.016*\"would\" + 0.011*\"people\" + 0.010*\"funny\" + 0.008*\"movie\"')\n"
     ]
    }
   ],
   "source": [
    "# try three topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model3.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.018*\"always\" + 0.013*\"instead\" + 0.011*\"really\" + 0.011*\"would\"')\n",
      "(1, '0.019*\"try\" + 0.012*\"machine\" + 0.011*\"never\" + 0.010*\"means\"')\n",
      "(2, '0.018*\"think\" + 0.015*\"funny\" + 0.013*\"would\" + 0.011*\"always\"')\n",
      "(3, '0.033*\"movie\" + 0.030*\"watch\" + 0.012*\"make\" + 0.010*\"night\"')\n",
      "(4, '0.017*\"could\" + 0.015*\"email\" + 0.014*\"place\" + 0.012*\"things\"')\n",
      "(5, '0.020*\"would\" + 0.013*\"something\" + 0.012*\"wrong\" + 0.012*\"sense\"')\n",
      "(6, '0.019*\"world\" + 0.018*\"remember\" + 0.012*\"clean\" + 0.010*\"dream\"')\n",
      "(7, '0.012*\"water\" + 0.012*\"problem\" + 0.012*\"coffee\" + 0.010*\"spend\"')\n",
      "(8, '0.022*\"people\" + 0.018*\"amaze\" + 0.013*\"every\" + 0.011*\"funny\"')\n",
      "(9, '0.016*\"change\" + 0.016*\"funny\" + 0.014*\"computer\" + 0.010*\"memory\"')\n"
     ]
    }
   ],
   "source": [
    "# try ten topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model10.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']\n",
      "11314\n",
      "[7 4 4 ..., 3 1 8]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Run LDA on Newsgroup Data\n",
    "# The Newsgroup Data\n",
    "# http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "texts = fetch_20newsgroups(subset='train')\n",
    "print(dir(texts))\n",
    "# 11,314 posts\n",
    "print(len(texts.target))\n",
    "print(texts.target)\n",
    "print(texts.target_names)\n",
    "print(texts.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
